---
layout: post
title: "Gradient Descent Method in Machine Learning"
tagline: "Steepest Descend Method"
description: "Steepest Descend Method"
category: projects
tags: [c++,machine learning]
excerpt: "Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent."
---
{% include JB/setup %}

## Question

We have some data which is structured using csv file. We have 248 parameters and one reference for each line of the data. This is the training set of your model. And also there will be a test set for you to test your data.

Competition Link: [linear-regression-sysu-2016](https://inclass.kaggle.com/c/linear-regression-sysu-2016/leaderboard)


## Gradient Descent

>Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.

>Gradient descent is also known as steepest descent, or the method of steepest descent. Gradient descent should not be confused with the method of steepest descent for approximating integrals.

*from Wikipedia*

![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/350px-Gradient_descent.svg.png)

## Mathematical Principle


## C++ Implentation

## N-fold Optimization
